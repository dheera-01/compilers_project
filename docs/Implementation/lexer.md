# Lexical Analysis

Our compiler reads the program that is steam of characters, then lexer convert there these stream of characters into tokens which is generated by my_lexer.py. This chapter describes how the lexical analyzer breaks a file into tokens.

## Stream Class

Source file is stream of characters, the `Stream` class represents a character stream of source file. It has two properties: `source` and `pos`. The `source` property is a string that contains the characters of program to be read, and the `pos` property is an integer that represents the current position in the string.

### Class Methods

The `Stream` class has three methods:

#### from_string(s: str) -> Stream

The `from_string` method is a class method that creates a new `Stream` object. It sets the `source` property to `s` and the `pos` property to 0.

#### next_char() -> str

The `next_char` method reads the current character from the stream and advances the `pos` property by 1 to go to next character. If the end of the stream is reached (when position is more than or equal to the length of stream), it raises an `EndOfStream` exception. It returns the current character.

#### unget()

The `unget` method decrements the `pos` property by 1 to go back one character. It raises an assertion error if the current position is already at the beginning of the stream.

##### Example Usage

Here's an example of how to use the `Stream` class:

```python
s = Stream.from_string("hello, world!")
while True:
    try:
        c = s.next_char()
        print(c, end = ‘’)
    except EndOfStream:
        break
s.unget()
s.next_char() # prints '!'
```

Output:

```
hello, world!
```

This code creates a Stream object from the string "hello, world!" and then reads and prints each character until the stream's end is reached. It then goes back one character using the unget method and reads and prints the exclamation mark at the end of the stream.

## Lexer Class

This part of a compiler breaks down source code into a stream of tokens, which can be processed by a parser. It has two properties, `stream` and `save`. `stream` stores the stream object form which token will be formed. `save` stores the current token and at the beginning it is set to `None`

#### from_stream(s):

Creates the new `Lexer` class object with `stream` property set to `s`

#### next_token(self) -> Token:

The `next_token()` method is the main method of the Lexer. It reads the next character from the stream and uses a number of rules to determine what kind of token it is. Tokens can be end-of-line markers, comments, operators, string literals, numbers, brackets, identifiers, or whitespace.

#### peek_current_token(self) -> Token:

The `peek_current_token()` method return the current token without advancing, it allows us to have a look on the curent token. At the start, when the current token is not there, it also sets the current token.

#### advance(self):

The `advance()` methods consume the current token and advance to the next token.

#### match(self, expected):

The `match()` methods matches the current token against the expected token and move one token ahead it if they match.

##### Example Usage

Here's an example of how to use the `Stream` class:

```python
program = "a = 5"
    for i in Lexer.from_stream(Stream.from_string(program)):
        print(i)
```

Output:

```
Identifier(a)
Operator(=)
NumLiteral(5)
```

### Tokens

All the tokens are formed by calling the `next_token()` method of `Lexer` class.

#### NumLiteral Token:

The `NumLiteral` is a datatype that denotes the Integer number of the compiler.
This is how we are creating the token of NumLiteral.

```python
case c if c.isdigit():
    n = int(c)
    while True:
        try:
            c = self.stream.next_char()
            if c.isdigit():
                n = n * 10 + int(c)
            elif c == ".":
                n = str(n)
                n = n + c
                while True:
                    c = self.stream.next_char()
                    if c.isdigit():
                        n += c
                    elif c == ".":
                        raise TokenError(
                            f"{n + c} Invalid number")
                    else:
                        self.stream.unget()
                        return FloatLiteral(float(n))
            else:
                self.stream.unget()
                return NumLiteral(n)
        except EndOfStream:
            return NumLiteral(n)
```

In `next_token()` method when `next_character` is a number `isdigit` will be true (`isnumeric` can also be used). Used while loop to read all the digit of this `NumLiteral` and simultaneously multiplying 10 to prevoiuos digit to get that number in integer for and `NumLiteral` stores the integer.

#### FloatLiteral Token:

The `FloatLiteral` is a datatype that denotes the floating point number of the compiler. There are two possibility of `FloatLiteral`, 12.6 or .56.
The first one is handled with `NumLiteral` as the digit before the decimal is `NumLiteral`. After the digit after the decimal are converted to string and then it is added at the back of string converted digits (before decimal + decimal). At the type of returning it is converted into float and then returned as a `FloatLiteral`.

To handle the second case of .56 here is the code.

```python
case c if c == ".":
    n = str(0)
    n = n + c
    while True:
        c = self.stream.next_char()
        if c.isdigit():
            n += c
        elif c == ".":
            raise TokenError(f"{n + c} Invalid number")
        else:
            self.stream.unget()
            return FloatLiteral(float(n))
```

Just added `0` as digit before the the decimal, and rest is same like the first case.

`Possible Error`:

If source has 12.36.96 with two decimals, it will give token error

#### EndOfLine Token:

`;` tells the compiler that end of line has been reached. It tell the compliant to stop parsing the current expression and start parsing the new expression that are after `;`. End of line must places after every expression else it will raise error. Strict checking of end of line is written in parser. Lexer just form tokens.

#### Operators

##### Symbolic Operators

Symbolic operator supported by out language

```python
symbolic_operators = "+ - * / % // ** < > <= >= == !=  << >> = += -= *= /= %= //= **=".split()
```

| Symbolic Operator                         | Use                                     | 
| -------------------------------- | ------------------------------------------------ | 
| `*`, `/`, `//`, `%`, `+`, `-`, `**`    |Arithmetic Opeariton (+ and - can be used for uniary operation also) | 
| `<`, `<=`, `>`, `>=`, `!=`, `==` | Comparisons operator                             | 
| `=`, `+=`, `-=`, `*=`, `/=`, `//=`, `**=`, `%=` | Assignment operator              | 


Except `!=` all the operator have the first character in the symbolic operators. For multilenght operator all its characters are in symbolic operator. We used this idea to get the tokens of each of these operators. After getting all the token, we checked again that combined operator is in the symbolic operator list (as =! Will pass all the above condition). If operator are not in the symbolic list (like `***`, `=!` It raise a error). It will return the `Operator` datatype, which is declared in the declaration.py

###### Handling the unary operator:

We allowed to enter `+-++-6` to be a valid expression. We handled these unary operators by adding a condition`s in “+-”`. It allows us to get the `+-++-` as a valid operator.

```python
case c if c in symbolic_operators:
start = self.stream.pos - 1
while True:
s = self.stream.next_char() # +- for unary operator ++--6
if s in symbolic_operators or s in "+-":
c = c + s
else:
self.stream.unget()
if c in symbolic_operators:
return Operator(c)
else:
for i in c:
if i not in "+-": # =! is not a valid operator
raise TokenError(f"{c} is an Invalid operator") # here getting unary operator
self.stream.pos = start + 1
return Operator(c[0])
````
###### Handling the != operator:

As `!` is not in the symbolic operator so it needs to be handled separately. While handling this, we have to reject the `!+` operator also.

```python
case c if c == "!":
s = self.stream.next_char()
if c + s in symbolic_operators:
return Operator(c + s)
raise TokenError(f"{c + s} is an Invalid operator")
```

##### Word Operators

```python
word_operators = "and or not is in".split()
```

Word are operators only but they are written in alphabetical characters, so handled with alphanumeric terms. They are tokenize inside the `word_to_token` function. It includes

| word operator |
| ---------------- |
| and |
|or|
|not|
|is|
|in|

#### StringLiteral Token:

What ever written between `“ ”` or `‘ ’` are considered as `StringLiteral`. We just iterate till we get `“` or `‘`. if not find we will get error of unterminated string literal. The code also handles the case when, it gets `“hello’` it will display unterminated string literal as it starts with double quotation marks but didn’t get the double quotation mark again. Compiler put a constraint that string literal starting and ending quotation marks in string needs to be same.

```python
case c if c == '"' or c == "'":
current_quote = c
st = ''
while True:
try:
c = self.stream.next_char()
if c == current_quote:
return StringLiteral(st)
else:
st = st + c
except EndOfStream:
raise TokenError(f"{st}: Unterminated string literal")
```

#### Brackets Token:
The `(`, `)`, `[`, `]`, `{`, `}` are the brackets tokens. With bracket we have implements the bracket matching also. The below code implements a bracket matching functionality. It ensures that all opening and closing brackets in a given input string match correctly.

`bracket_track_list`: A list that tracks all the opening brackets seen so far.

`bracket_map`: A dictionary that maps each closing bracket to its corresponding opening bracket.

```python
bracket_track_list = []
bracket_map = {')': '(', '}': '{', ']': '['}
```

For opening brackets, the code appends them to the `bracket_track_list` and returns a `Bracket` object with the same character.

For closing brackets, the code checks if the `bracket_track_list` is empty or if the most recent opening bracket on the list matches the current closing bracket using the `bracket_map`. If a match is found, the code returns a Bracket object with the same character. If not, it raises a `TokenError` with a message indicating the unmatched closing bracket.


``` python 
# bracket and bracket matching
case c if c in opening_brackets:
bracket_track_list.append(c)
return Bracket(c)
case c if c in closing_brackets:
temp = c
if len(bracket_track_list) == 0 or bracket_map[c] != bracket_track_list.pop():
print(Bracket(c))
raise TokenError(f"{c} Unmatched closing bracket")
return Bracket(c)
```

##### Keyword Tokens:
The allowed keywords are:

```python
keywords = """
    int string float const assign slice
    if elif else break continue
    for while break continue
    def
    print let 
    slice in 
    """.split()
```


#### BoolLiteralToken:
The `False` and `True` are considered as BoolLiteral token. They are handled with `word_to_token` function.

#### Identifier Token:

Any alphanumeric character stream that are not in quotation marks, word operators, bool literal and keyword will be identifiers. Even underscore `_` are allowed are identifier.
For alphabetic characters and underscores, the code starts a loop that reads in the rest of the characters until a non-alphanumeric character is reached. It then passes the resulting word to the `word_to_token` function to convert it to the corresponding token type.

```python
def word_to_token(word):
"""Convert a word to a tokens. Tokens are keywords, word operators, bool literals, identifiers"""
if word in keywords:
return Keyword(word)
if word in word_operators:
return Operator(word)
if word == "True":
return BoolLiteral(True)
if word == "False":
return BoolLiteral(False)
return Identifier(word)
```

```python
# reading the identifiers

# _, a are valid identifiers

case c if c.isalpha() or c == "_":
s = c
while True:
try:
c = self.stream.next_char() # a1, a_ is valid identifier
if c.isalpha() or c == "\_" or c.isdigit():
s = s + c
else:
self.stream.unget()
return word_to_token(s)
except EndOfStream:
return word_to_token(s)
```

### Comments

This is not tokens but it is the job of lexer to remove the comments from the code. A comment starts with a hash character `#`, and whatever written after the this till the new line character (`‘\n’`) will be comment it will be removed from code and will not go to parser. Still we are saving the comments in the global list `comments` as these might come in handy in future!!
Once comment reading is done, we are reading the next token because whenever we call next token we want something but token are not giving anything so going to next token is required.
This is how we have implemented the comment statement.

```python
case c if c == "#":
cmt = ""
while True:
c = self.stream.next_char()
if c == "\n": # one line comment ends
comments.append(cmt) # lexer removes the comments and moves on
return self.next_token()
cmt = cmt + c
```

### Whitespaces
Tab, new line and and any unit of space are considered as whitespace and they are neglected by lexer
```python
program = '''5 +  
6'''
for i in Lexer.from_stream(Stream.from_string(program)):
print(i)
```

Output:
```python
NumLiteral(5)
Operator(+)
NumLiteral(6)
```

### Example Usage

```python
program = 'if 5 >= .2 != "hello" and True False { } a = 5;'
for i in Lexer.from_stream(Stream.from_string(program)):
print(i)
```

Output:
```python
Keyword(if)
NumLiteral(5)
Operator(>=)
FloatLiteral(0.2)
Operator(!=)
StringLiteral("hello")
Operator(and)
BoolLiteral(True)
BoolLiteral(False)
Bracket({)
Bracket(})
Identifier(a)
Operator(=)
NumLiteral(5)
EndOfLine(;)
```
