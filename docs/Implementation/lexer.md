# Lexical Analysis

Our compiler reads the program that is steam of characters, then lexer convert there these stream of characters into tokens which is generated by lexer.py. This chapter describes how the lexical analyzer breaks a file into tokens.

## Stream Class
Source file is stream of characters, the `Stream` class represents a character stream of source file. It has two properties: `source` and `pos`. The `source` property is a string that contains the characters of program to be read, and the `pos` property is an integer that represents the current position in the string.

### Class Methods
The `Stream` class has three methods:

#### from_string(s: str) -> Stream
The `from_string` method is a class method that creates a new `Stream` object. It sets the `source` property to `s` and the `pos` property to 0.

#### next_char() -> str
The `next_char` method reads the current character from the stream and advances the `pos` property by 1 to go to next character. If the end of the stream is reached (when position is more than or equal to the length of stream), it raises an `EndOfStream` exception. It returns the current character.

#### unget()
The `unget` method decrements the `pos` property by 1 to go back one character. It raises an assertion error if the current position is already at the beginning of the stream.


##### Example Usage
Here's an example of how to use the `Stream` class:

```python
s = Stream.from_string("hello, world!")
while True:
    try:
        c = s.next_char()
        print(c, end = ‘’)
    except EndOfStream:
        break
s.unget()
s.next_char() # prints '!'
```
Output:
```
hello, world!
```

This code creates a Stream object from the string "hello, world!" and then reads and prints each character until the stream's end is reached. It then goes back one character using the unget method and reads and prints the exclamation mark at the end of the stream.


## Lexer Class
This part of a compiler breaks down source code into a stream of tokens, which can be processed by a parser. It has two properties, `stream` and `save`. `stream` stores the stream object form which  token will be formed. `save` stores the current token and at the beginning it is set to `None`

#### from_stream(s):
Creates the new `Lexer` class object with `stream` property set to `s`

#### next_token(self) -> Token: 
The `next_token()` method is the main method of the Lexer. It reads the next character from the stream and uses a number of rules to determine what kind of token it is. Tokens can be end-of-line markers, comments, operators, string literals, numbers, brackets, identifiers, or whitespace.

#### peek_current_token(self) -> Token: 
The `peek_current_token()` method return the current token without advancing, it allows us to have a look on the curent token. At the start, when the current token is not there, it also sets the current token.

#### advance(self): 
The `advance()` methods consume the current token and advance to the next token. 

#### match(self, expected):
The `match()` methods matches the current token against the expected token and move one token ahead it if they match.

##### Example Usage
Here's an example of how to use the `Stream` class:

```python
program = "a = 5"
    for i in Lexer.from_stream(Stream.from_string(program)):
        print(i)
```
Output:
```
Identifier(a)
Operator(=)
NumLiteral(5)
```

## Lexer Class

All the tokens are formed by calling the `next_token()` method of `Lexer` class.

###
